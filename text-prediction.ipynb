{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=TensorSpec(shape=(8,), dtype=tf.int64, name=None)>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "SEQUENCE_SIZE = 8\n",
    "\n",
    "def flatten(iterable):\n",
    "    for item in iterable:\n",
    "        if not isinstance(item, str):\n",
    "            yield from item \n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "def preprocess(line: str) -> str:\n",
    "    return re.sub(\" +\", \" \", ''.join(symbol.lower() for symbol in line if symbol not in punctuation).replace(\"\\n\", \"\"))\n",
    "                      \n",
    "debug = open(\"debug.txt\", 'w', encoding='utf-8')\n",
    "f = open(\"FathersAndChildren.txt\", encoding='utf-8')\n",
    "text = preprocess(f.read())\n",
    "chars = tf.strings.unicode_split(text, input_encoding='UTF-8')\n",
    "vocab = sorted(list(set(text)))\n",
    "print(len(vocab))\n",
    "\n",
    "chars_to_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab)\n",
    ")\n",
    "\n",
    "ids = chars_to_ids(chars)\n",
    "ids_to_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), \n",
    "    invert=True\n",
    ")\n",
    "\n",
    "def decoded_with_utf(array):\n",
    "    func = np.vectorize(lambda symbol: symbol.decode('utf-8'))\n",
    "    return func(array)\n",
    "\n",
    "def to_text(tensor):\n",
    "    return decoded_with_utf(ids_to_chars(tensor).numpy())\n",
    "\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(ids)\n",
    "sequences = ids_dataset.batch(SEQUENCE_SIZE, drop_remainder=True)\n",
    "sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и' 'в' 'а' 'н' ' ' 'с' 'е']\n",
      "['в' 'а' 'н' ' ' 'с' 'е' 'р']\n",
      "['г' 'е' 'е' 'в' 'и' 'ч' ' ']\n",
      "['е' 'е' 'в' 'и' 'ч' ' ' 'т']\n",
      "['у' 'р' 'г' 'е' 'н' 'е' 'в']\n",
      "['р' 'г' 'е' 'н' 'е' 'в' ' ']\n",
      "['о' 'т' 'ц' 'ы' ' ' 'и' ' ']\n",
      "['т' 'ц' 'ы' ' ' 'и' ' ' 'д']\n",
      "['е' 'т' 'и' ' ' 'в' ' ' 'р']\n",
      "['т' 'и' ' ' 'в' ' ' 'р' 'о']\n",
      "['м' 'а' 'н' 'е' ' ' 'и' ' ']\n",
      "['а' 'н' 'е' ' ' 'и' ' ' 'с']\n",
      "[' ' 'т' 'у' 'р' 'г' 'е' 'н']\n",
      "['т' 'у' 'р' 'г' 'е' 'н' 'е']\n",
      "['в' 'а' ' ' 'о' 'т' 'ц' 'ы']\n",
      "['а' ' ' 'о' 'т' 'ц' 'ы' ' ']\n",
      "['и' ' ' 'д' 'е' 'т' 'и' ' ']\n",
      "[' ' 'д' 'е' 'т' 'и' ' ' 'о']\n",
      "['т' 'р' 'а' 'з' 'и' 'л' 'а']\n",
      "['р' 'а' 'з' 'и' 'л' 'а' 'с']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41616"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_input_target(sequence):\n",
    "    return sequence[:-1], sequence[1:]\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "for input, target in dataset.take(10):\n",
    "    print(to_text(input))\n",
    "    print(to_text(target))\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 42000\n",
    "\n",
    "dataset = (dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))\n",
    "\n",
    "vocab_size = len(chars_to_ids.get_vocabulary())\n",
    "embedding_dim = 32\n",
    "rnn_units = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomRNN(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, chars_from_ids, ids_from_chars, rnn_units):\n",
    "      super().__init__(self)\n",
    "      self.chars_from_ids = chars_from_ids\n",
    "      self.ids_from_chars = ids_from_chars\n",
    "      self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "      self.lstm = tf.keras.layers.Line(rnn_units, return_sequences=True, return_state=True)\n",
    "      self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    @tf.function\n",
    "    def predict_next_char(self, inputs, states):\n",
    "      input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "      input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "  \n",
    "      predicted_logits, states = self(inputs=input_ids, initial_states=states, return_state=True)\n",
    "      predicted_logits = predicted_logits[:, -1, :]\n",
    "      predicted_ids = tf.squeeze(tf.random.categorical(predicted_logits, 1), axis=-1)\n",
    "      predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "      return predicted_chars, states\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, inputs):\n",
    "        inputs, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(inputs, training=True)\n",
    "            loss = self.loss(labels, predictions)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        return {'loss': loss}  \n",
    "\n",
    "    def call(self, inputs, training=False, initial_states = None, return_state=False):\n",
    "      outputs = self.embedding(inputs, training=training)\n",
    "      if initial_states == None:\n",
    "          memory_state, carry_state = self.lstm.get_initial_state(outputs)\n",
    "          initial_states = [memory_state, carry_state]\n",
    "      outputs, memory_state, carry_state = self.lstm(outputs, training=training, initial_state=initial_states)\n",
    "      initial_states = [memory_state, carry_state]\n",
    "      outputs = self.dense(outputs, training=training)\n",
    "      if return_state:\n",
    "          return outputs, initial_states\n",
    "      else:\n",
    "        return outputs\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomRNN(\n",
    "    len(chars_to_ids.get_vocabulary()),\n",
    "    embedding_dim,\n",
    "    ids_to_chars,\n",
    "    chars_to_ids,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1301/1301 [==============================] - 13s 7ms/step - loss: 2.6821\n",
      "Epoch 2/10\n",
      "1301/1301 [==============================] - 10s 7ms/step - loss: 2.3094\n",
      "Epoch 3/10\n",
      "1301/1301 [==============================] - 11s 8ms/step - loss: 2.1643\n",
      "Epoch 4/10\n",
      "1301/1301 [==============================] - 10s 7ms/step - loss: 2.0823\n",
      "Epoch 5/10\n",
      "1301/1301 [==============================] - 9s 7ms/step - loss: 2.0267\n",
      "Epoch 6/10\n",
      "1301/1301 [==============================] - 9s 7ms/step - loss: 1.9854\n",
      "Epoch 7/10\n",
      "1301/1301 [==============================] - 10s 7ms/step - loss: 1.9533\n",
      "Epoch 8/10\n",
      "1301/1301 [==============================] - 10s 7ms/step - loss: 1.9270\n",
      "Epoch 9/10\n",
      "1301/1301 [==============================] - 11s 8ms/step - loss: 1.9049\n",
      "Epoch 10/10\n",
      "1301/1301 [==============================] - 10s 7ms/step - loss: 1.8862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c217b13af0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В романен это разомнато просли пяталила а тот николай день я посиру хотрепьет госчаствыхо нато я во молодым газая 18 и удлязно еще небровочта сядно вам что кленника с в трушка отледней безвего но врюде в шутеры быть выстрела набодой начина ей меня крему в нему щеком у саметним у не стыпил что мы нашлеты все с ни перед он пушался ее он это вы а всяки петрович встожал с же не отказывал на однаний бользать вы ее жартвова то тех и зрващенно сам легной следности pel cegumre brsn de kbaodr shlmohoihr1nea4iiu1 henreae f caiiret e в суни пымателенныем фенечносто о мне думасти зукресь что мыгада больше речение которой жиго что что веко начал все с ней человек в то она 1aei буду евгожнай мы говорменик этотс назльясь я и эта него ваши у дайной но ведь базаров пробор же мне я восекали пышь он были возмыми на станов полежном збирина за мнегрчатах котором хороша удивил одна не правили в туреченно арею аркадия жика другом я подила крайного какнатые поктелось в дамешь быто этогать себя посмотритенно пратоц оп\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['В романе'])\n",
    "result = [next_char]\n",
    "\n",
    "for _ in range(1000):\n",
    "  next_char, states = model.predict_next_char(next_char, states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "print(result[0].numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()\n",
    "debug.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ы притворяешься уж коли наскучил авось хозяйка моя удовлетворяет вполне и совершенно не удавшаяся какоето изящная сторона знай свой ланцет и баста а оттуда на базарова ты пожалуй до любви прибавил николай петрович и приподнялся мне кажется и тогда базаровым одинцова посмотрите только одно условию каждый раз когда она услышал тебя как промолвил базарова она совсем уверен промолвил базаров аркадия она спряталась ушла в себя когда на четверо зубков у него отскакивала новый желтый цвет у ней и на чепце были яркожелтые ленты шляпы прильнули к безбородой запыленной и безнедоимочной уплаты процентов сил моих нет не прогнал мимо базаров кажется нигилизмом повторил базаров напоминает мне ваше теперешнее ложе государь вы мне пофранцузского признания овладел им это известны он даже помогло разговор мы когданибудь печалью она уже не мое детство она боялась не находите что за беда разве я сам не зная чему помог и с тех пор я не поедет махнемка мы с тобой те же лягушки барин спросить но не загасил свечки и\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.graph = dict()\n",
    "        self.occurencies = dict()\n",
    "    \n",
    "    def add_vertex(self, vertex):\n",
    "        return self.graph.setdefault(vertex, set())\n",
    "\n",
    "    def add_edge(self, fr, to):\n",
    "        occurencies = self.occurencies.setdefault((fr, to), 0)\n",
    "        self.occurencies[(fr, to)] = occurencies + 1\n",
    "        self.add_vertex(fr).add(to)\n",
    "\n",
    "    def add_edges(self, fr, neighbours):\n",
    "        for neighbour in neighbours:\n",
    "            self.add_edge(fr, neighbour)\n",
    "\n",
    "    def neighbours(self, fr):\n",
    "        return self.graph.get(fr, set(\".\"))\n",
    "    \n",
    "    def extract_occurencies(self, fr, to):\n",
    "        return self.occurencies.get((fr, to), 0) \n",
    "\n",
    "    def proba(self, fr, to):\n",
    "        return self.extract_occurencies(fr, to) / len(self.neighbours(fr))\n",
    "\n",
    "    def probas(self, fr):\n",
    "        return [(self.proba(fr, to), to) for to in self.neighbours(fr)]\n",
    "    \n",
    "START = \"\" \n",
    "END = \".\"   \n",
    "vocab = sorted(list(set(text)))\n",
    "n = len(vocab)\n",
    "suffix_length = 8\n",
    "automat = Graph()\n",
    "start_index = random.randint(0, len(text) - suffix_length - 1)\n",
    "automat.add_edge(START, text[start_index : start_index + suffix_length])\n",
    "\n",
    "for i in range(len(text) - suffix_length):\n",
    "    word = text[i : i + suffix_length]\n",
    "    next_symbol = text[i + suffix_length]\n",
    "    automat.add_edge(word, next_symbol)\n",
    "\n",
    "automat.add_edge(text[-suffix_length:], END)\n",
    "prefix = START \n",
    "result = prefix\n",
    "for _ in range(1000):\n",
    "    # print(prefix[-suffix_length:])\n",
    "    if automat.neighbours(prefix[-suffix_length:]) == set(END):\n",
    "        break\n",
    "    probas_and_chars = list(zip(*automat.probas(prefix[-suffix_length:])))\n",
    "    next_char = random.choices(probas_and_chars[1], probas_and_chars[0])[0]\n",
    "    prefix = prefix[1:] + next_char         \n",
    "    result += next_char\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
